# -*- coding: utf-8 -*-
"""Stack_Exchange_Quality_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iOVXl40DRgthR-Q31eQuAfAzAPzHyRNz

**Import Libraries**
"""

import xml.etree.ElementTree as ET
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import nltk
nltk.download('punkt')
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc
from imblearn.combine import SMOTETomek
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score, confusion_matrix

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

"""# **1.Dataset Creation**

We aim to extract data from an XML file and save it in a properly structured CSV format. To achieve this, we utilized the ElementTree library for parsing and extracting data from the XML file. The extracted data is then stored in a file named dataset.csv.
"""

#Parse the xml file
tree = ET.parse('/content/drive/MyDrive/New Stack Exchange/physics.stackexchange.com(1)/Posts.xml')
root = tree.getroot()

# define the columns of the dataset
columns = ['Id', 'PostTypeId', 'AcceptedAnswerId', 'Score', 'ViewCount', 'Tags', 'Title', 'Body', 'AnswerCount', 'CommentCount', 'FavoriteCount']

# create an empty list to store the data
data = []

for row in root.findall('.//row'):

  id_val = row.get('Id')
  post_type_id = row.get('PostTypeId')
  accepted_answer_id = row.get('AcceptedAnswerId')
  score = row.get('Score')
  Title = row.get('Title')
  view_count = row.get('ViewCount')
  Tags = row.get('Tags')
  body = row.get('Body')
  answer_count = row.get('AnswerCount')
  comment_count = row.get('CommentCount')
  favorite_count = row.get('FavoriteCount')

  # Append the attribute values as a list to the data list
  if int(post_type_id) == 1:
    data.append([id_val, post_type_id, accepted_answer_id, score, view_count, Tags, Title, body, answer_count, comment_count, favorite_count])


df = pd.DataFrame(data, columns=columns)

df.to_csv('dataset.csv', index=False)
print('dataset.csv created ')

"""2. To handle the large size of the dataset, we are employing Simple Random Sampling to create a representative subset of the data."""

df = pd.read_csv('dataset.csv', encoding='iso-8859-1')

# randomly sample 50000 rows from the dataframe
sample = df.sample(n=50000, replace=False, random_state=42)

sample.to_csv('sampled_dataset.csv')
print('sampled_dataset created')
print('Shape of dataset',sample.shape)

"""**3. Create complete dataset**

We need to generate dependent features based on the independent features and the answer count, applying specific conditions to define the relationships.
"""

df = pd.read_csv('sampled_dataset.csv')

#create a new column that labels questions as 'good-quality', 'low quality', 'very-low quality'
def label_question(row):
  if row['Score'] > 5 and row['AnswerCount'] > 0:
    return 'good quality'
  elif row['Score'] <= 5 and row['Score'] >= 0 and row['AnswerCount'] >= 0:
    return 'low quality'
  elif row['Score'] < 0:
    return 'very low quality'
  else:
    return 'unknown'

df['question_quality'] = df.apply(label_question, axis=1)

# convert dataframe to csv file
df.to_csv("complete_dataset.csv")
print("complete_dataset created")

data = pd.read_csv('complete_dataset.csv')

"""# **2. Data Cleaning**

We are dropping irrelevant or unimportant columns that are not being used in the analysis. While columns like AnswerCount and Score were initially used to create dependent features, they are no longer needed and are therefore being removed.
"""

col = ['Unnamed: 0', 'Unnamed: 0.1', 'Id', 'PostTypeId', 'AcceptedAnswerId', 'AnswerCount', 'Score']
data.drop(col, inplace=True, axis=1)

"""We are creating features for the length of the Title and Body data. Additionally, we are cleaning the Body text column by removing tags and unwanted characters."""

# Remove <p> and </p> tags from the 'Body' column
data['Body'] = data['Body'].str.replace('<p>', '').str.replace('</p>', '')
data.head()

"""We are adding new features for the Title and Body columns, specifically the character count and word count for each."""

#calculate the character count and word count for 'Title' column and 'Body' column
data['Title_Text_Length'] = data['Title'].str.len()

data['Body_Text_Length'] = data['Body'].str.len()

data['Body_Sentences_Count'] = data['Body'].apply(lambda x: len(str(x).split('.')) if isinstance(x, str) else 0)
data.head()

df = data.copy()

"""We are dropping the Title and Body columns, as well as the Tags column. Upon evaluation, we found that the Tags column increases complexity without contributing to improved accuracy."""

col = ['Title', 'Body', 'Tags']
data.drop(col, inplace=True, axis=1)

data.isnull().sum()

"""Since more than 50% of the data in the FavoriteCount column is null, we have decided to drop this column."""

data.shape

data.drop('FavoriteCount', inplace=True, axis=1)
data.shape

"""**Check Duplicates**"""

data.duplicated().sum()

data.describe()

"""Although outliers are present in the data, we are not removing them in this case. These outliers could represent high-quality questions with longer, well-explained text or, conversely, low-quality questions. Removing them might lead to the loss of valuable insights.

check data type of dataset
"""

data.dtypes

"""The data type for the question_quality column should be set to "object," while all other columns should be of integer or float type."""

data

"""# **3. Exploratory Data Analysis**

3.1 Observe dependent column
"""

print('Dataset shape: ', data.shape)

data['question_quality'].value_counts()

#percentage of class present in target variable(y)
print("percentage of low_quality, good_quality and very low quality \n", data['question_quality'].value_counts()/len(data)*100)

plt.style.use('ggplot')

plt.figure(figsize=(20,4))
ax = plt.subplot(121)
data['question_quality'].value_counts(normalize=True).plot(kind='bar')
plt.xlabel('question_quality')
plt.ylabel('percentage of question_quality')
plt.title('question_quality')

#Add percentage labels to the top of each bar
for p in ax.patches:
  ax.text(p.get_x()+p.get_width()/2., p.get_height()+0.02, f"{p.get_height()*100:.1f}%", ha="center")

"""The data in the question_quality column is imbalanced, with a distribution of 82.8% for one class, 11% for another, 5.8% for a third, and only 0.3% for the "unknown" category. Since the "unknown" data is minimal, we can either remove or replace it."""

data = data[data['question_quality'] != 'unknown']

data.replace({'question_quality' : { 'very low quality' : 0, 'good quality' : 2, 'low quality' : 1 }},inplace=True)

"""3.2 Univariate Analysis"""

# plotting histogram for each numerical variable
column_names = list(data.columns)
column_names.remove('question_quality')
plt.style.use("ggplot")
for column in column_names:
    plt.figure(figsize=(20,4))
    plt.subplot(121)
    sns.distplot(data[column], kde=True, rug=True)
    plt.title(column)

"""Most of the columns exhibit skewed data, with the exception of Title_char_count and Title_word_count.

Since the outliers in this case are valuable, we cannot remove them, and thus, addressing skewness is not feasible.

3.3 correlation
"""

plt.figure(figsize=(25,15))
sns.heatmap(data.corr(), annot=True)

"""#**4. Split dataset into train and test**"""

X = data.drop(['question_quality'], axis=1)
y = data['question_quality']

'''from imblearn.over_sampling import SMOTENC

cat_features = np.array([0, 1, 2])
# Define the SMOTENC object with a sampling strategy for each class
sampling_strategy = {0: 20000, 1: 44000, 2: 20000}
smote_nc = SMOTENC(sampling_strategy=sampling_strategy, categorical_features=cat_features, random_state=42)

# Apply SMOTE-NC oversampling to the training data
X_resampled, y_resampled = smote_nc.fit_resample(p, q)

# Check the class distribution of the resampled data
print(y_resampled.value_counts())'''

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

"""# **5. Scaling**"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""# **6.Model**

6.1 Logistic Regression
"""

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)

train_predicted = lr_model.predict(X_train)
test_predicted = lr_model.predict(X_test)

print("The accuracy of Logistic Regression on train data is: ", accuracy_score(y_train, train_predicted)*100, "%")
print("The accuracy of Logistic Regression on test data is: ", accuracy_score(y_test, test_predicted)*100, "%")

print("\nPrecision : ", precision_score(y_test, test_predicted, average='weighted'))
print("Recall : ",recall_score(y_test, test_predicted, average='weighted'))
print("F1-score : ", f1_score(y_test, test_predicted, average='weighted'))

"""These results show that the Logistic Regression model is performing consistently on both the training and test sets, achieving accuracies of around 83.5%. This indicates that the model is not overfitting to the training data and is generalizing well to new, unseen data.

6.2 RandomForestClassifier
"""

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=0)
rf_model.fit(X_train, y_train)
train_predicted = rf_model.predict(X_train)
test_predicted = rf_model.predict(X_test)

print("The accuracy of Random Forest on train data is : ", accuracy_score(y_train, train_predicted)*100, "%")
print("The accuracy of Random Forest on test data is : ", accuracy_score(y_test, test_predicted)*100, "%")

test_precision =  precision_score(y_test, test_predicted, average='weighted')
test_recall = recall_score(y_test, test_predicted, average='weighted')
test_f1 =  f1_score(y_test, test_predicted, average='weighted')

print("Precision (test): ", test_precision)
print("Recall (test): ", test_recall)
print("F1-score (test): ", test_f1)

importances = rf_model.feature_importances_
df1 = pd.DataFrame({"Features":pd.DataFrame(X_test).columns, "importances":importances})
df1.set_index("importances")

df1 = df1.sort_values('importances')

"""The results reveal that the Random Forest Classifier model achieves perfect precision, recall, and F1-score on the training set, but slightly lower performance on the test set. This suggests potential overfitting to the training data.

However, the precision, recall, and F1-score values on the test set are crucial for assessing the model's performance on new, unseen data. With an F1-score of 0.78 on the test set, the model demonstrates reasonable overall performance, though there is still room for improvement.

6.3 Cross validation
"""

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Define the number of folds
n_folds = 3

# Define the multi-class classification model
model = RandomForestClassifier()

# Define the cross-validation model
skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)

# Perform cross-validation
scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')

# Print the scores for each fold
print("Cross-validation scores:", scores)

# Calculate the mean and standard deviation of the scores
print("Mean accuracy:", np.mean(scores))
print("Standard deviation:", np.std(scores))

from imblearn.over_sampling import SMOTE
from sklearn.feature_extraction.text import CountVectorizer

count_vec = CountVectorizer(max_features=10000)
bow = count_vec.fit_transform(df['Body'])
X = bow
y = df['question_quality']

# split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)

#create an instance of the SMOTE class
smote = SMOTE()

#fit and apply smote to the training data only
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

#convert the resampled X_train to a dense numpy array
X_train_resampled = X_train_resampled.toarray()
#train your model on the resampled training data
model = MultinomialNB().fit(X_train_resampled, y_train_resampled)


#evaluate the model on test set

train_predicted = model.predict(X_train)
test_predicted = model.predict(X_test)
test_precision = precision_score(y_test, test_predicted, average='weighted')
test_recall = recall_score(y_test, test_predicted, average='weighted')
test_f1 = f1_score(y_test, test_predicted, average='weighted')

print("Precision (test): ", test_precision)
print("Recall (test): ", test_recall)
print("F1-score (test): ", test_f1)

importances = rf_model.feature_importances_
# Ensure the number of columns in X_test matches the feature importances
if X_test.shape[1] == len(importances):
    df1 = pd.DataFrame({
        "Features": X_test.columns,
        "importances": importances
    })
    df1.set_index("Features", inplace=True)
    df1 = df1.sort_values('importances')
    df1.plot.bar(color='teal', title="Feature Importances", legend=False)
else:
    print("Mismatch between number of features in X_test and length of feature importances.")

"""Random Forest:

    Train Accuracy: 100%
    Test Accuracy: 83.12%
    Train Precision, Recall, and F1-score: 1.0, 1.0, 1.0
    Test Precision, Recall, and F1-score: 0.76, 0.83, 0.78

Logistic Regression:

    Train Accuracy: 83.45%
    Test Accuracy: 83.53%
    Train Precision, Recall, and F1-score: 0.76, 0.83, 0.77
    Test Precision, Recall, and F1-score: 0.77, 0.84, 0.77

Multinomial Naive Bayes with SMOTE:

    Test Accuracy: 70.87%
    Test Precision: 0.73
    Test Recall: 0.70
    Test F1-score: 0.72

Based on accuracy and F1-score, Random Forest and Logistic Regression outperform Multinomial Naive Bayes. Random Forest achieves the highest train accuracy and perfect precision, recall, and F1-scores on the training set, along with strong performance on the test set. Logistic Regression also performs well, with similar accuracy and F1-score, and its precision and recall on the test set surpass those of Random Forest.

**Feature Importance**
"""

feature_names = list(data.columns)
print(feature_names)
if 'question_quality' in feature_names:
  feature_names.remove('question_quality')
total_importance = importances.sum()
percent_importances = importances / total_importance * 100

# Create a dataframe with the feature names and their percent importances
df = pd.DataFrame({"feature_names": feature_names, 'percent_importances' : percent_importances})

# Sort the dataframe by percent importances in descending order
df = df.sort_values(by='percent_importances', ascending=False)

# Create a bar plot of the percent importances
plt.bar(x=df['feature_names'], height=df['percent_importances'], color='teal')
plt.xticks(rotation=90)
plt.xlabel('Feature')
plt.ylabel('Percent Importance')
plt.show()

"""# **Conclusion**

Logistic regression is a widely used and effective classification algorithm, particularly suited for situations where there is a linear relationship between the input variables and the output. In this case, logistic regression achieved strong accuracy on both the training and test datasets, as well as solid performance in terms of precision, recall, and F1-score.

Moreover, logistic regression is a simple and interpretable model, which is advantageous when understanding the relationship between input variables and the output is crucial. It is also computationally efficient and can handle large datasets with ease.

Overall, logistic regression proved to be a strong choice for this problem, offering solid performance while maintaining simplicity and interpretability.
"""

